Lesson-01 Assignment

########################################################################################################################################
1. 复现课堂代码部分：

adj_grammar = """
Adj* => null | Adj Adj*
Adj =>  蓝色的 | 好看的 | 小小的
""" 蓝色的 | 好看的 | 小小的
"""

# 以上部分是采用语言学家使用的语法规则，通过手动的方式编译出来的一套语法。按照词性进行划分的。目的是根据这样的内容，生成一句话如何来做？
# 第一步：在单一的词性中，如果随机选择一个词他是 adj (只能选择一个词，没法扩展的，叫终结词terminal）
import random
def adj():
    return random.choice("蓝色的 | 好看的 | 小小的".split('|')).split()[0]
 
adj()
>>>蓝色的
>>>好看的
>>>小小的
...

# 第二步：在单一的词性中，如果随机选择一个词他是 adj*，表示这样的词语叫做非终结词语，可以扩展，一般会出现在等号的左边（虽然再一句话中看到的再右边，但是再左边是可以找到的。（类似于递归调用）
def adj_star():
    return random.choice([lambda:'', lambda: adj() + adj_star()])()
    
adj_star()
>>>蓝色的蓝色的
>>>好看的蓝色的
>>>小小的蓝色的好看的
>>>小小的小小的小小的
>>>小小的
...   


综上，一般地：
adj_grammar = """
Adj* => null | Adj Adj*
Adj =>  蓝色的 | 好看的 | 小小的
"""

# 可以将以上部分转化成字典的数据结构。
grammar = {}
for line in adj_grammar.split('\n'):
    if not line.strip():
        countinue;
    a,b = line.split('=>')
    grammar[a.strip()] = [s for s.split() in b.split('|')]   

grammar
>>>{'Adj*': [['null'], ['Adj', 'Adj*']], 'Adj': [['蓝色的'], ['好看的'], ['小小的']]}
grammar[Adj]
>>> [['蓝色的'], ['好看的'], ['小小的']]
grammar[Adj*]
>>>[['null'], ['Adj', 'Adj*']]


#推广来说：对于：给定的一段具有表示含义的字符串，输入的是该字符串，输出的是需要具备含有字典数据结构的语法结构，方便后面使用。
类似于：
simple_grammar = """
sentence => noun_phrase verb_phrase
noun_phrase => Article Adj* noun
Adj* => null | Adj Adj*
verb_phrase => verb noun_phrase
Article =>  一个 | 这个
noun =>   女人 |  篮球 | 桌子 | 小猫
verb => 看着   |  坐在 |  听着 | 看见
Adj =>  蓝色的 | 好看的 | 小小的
"""

使用：
def grammar(grammar_str)
    grammar = {}
    for line in adj_grammar.split('\n'):
        if not line.strip():
            countinue;
        a,b = line.split('=>')
        grammar[a.strip()] = [s for s.split() in b.split('|')]
    return grammar
    
example_grammar = grammar(simple_grammar)
example_grammar
>>>{'sentence': [['noun_phrase', 'verb_phrase']],
 'noun_phrase': [['Article', 'Adj*', 'noun']],
 'Adj*': [['null'], ['Adj', 'Adj*']],
 'verb_phrase': [['verb', 'noun_phrase']],
 'Article': [['一个'], ['这个']],
 'noun': [['女人'], ['篮球'], ['桌子'], ['小猫']],
 'verb': [['看着'], ['坐在'], ['听着'], ['看见']],
 'Adj': [['蓝色的'], ['好看的'], ['小小的']]}


#综上：根据字符串===》》》产生了具有字典形式的数据结构===》》》如果是不可扩展的，直接返回，如果是可以扩展的直接再次进入扩展。最终可以生成一个句子。
#逻辑过程：如果是再左边的，那么直接随机选择，如果选择出来右边的是终结符，直接返回，如果选择出来的是非终结符，可以扩展的，那么再次调用每一个扩展，依次类推，直到能选择出终结符为止。
import random
def generate(gram, target):
    if target not in gram:
        return target
    expanded = [generate(gram,t) for t in ramdon.choice(gram[target])]
    return ''.join(e for e in expanded)


#综合上述过程：给定了语法规则，给定了一段语句，理想情况下，也就是输入一个该字典形式的数据结构grammar跟一段随机的符合语法结构的句子，输出的就是他随机产生的一个句子，如何做？
例如：
{'sentence': [['noun_phrase', 'verb_phrase']],
 'noun_phrase': [['Article', 'Adj*', 'noun']],
 'Adj*': [['null'], ['Adj', 'Adj*']],
 'verb_phrase': [['verb', 'noun_phrase']],
 'Article': [['一个'], ['这个']],
 'noun': [['女人'], ['篮球'], ['桌子'], ['小猫']],
 'verb': [['看着'], ['坐在'], ['听着'], ['看见']],
 'Adj': [['蓝色的'], ['好看的'], ['小小的']]}
 
 import random
 def generate(gram,target):
     if target not in gram:
         return target
     expanded = [generate(gram,t) for t in random.choice(gram[target])
     return ''.join([e for e in expanded if e !='null'])
  generate(gram=example_grammar,'sentence')
  
  >>> '一个好看的小猫看着这个女人'
  
  
  
 ##以上部分就是rule based的paradigm范例实际上就是自动机，意思是说，要给定一个语言规则，以及一个根据语法规则设计好的一个语句，通过判断，是否为终结符进行递归的判断，
 ##最终生成一句话。目前应用比较广的是再机器语言的编译上面。
 
 问题来了： 这样的一句话，是否真的有意义，以及逻辑是否正确呢，因为他是随机选择的，再匹配的过程中一定有一些句子，是逻辑混乱的。所以针对这个问题，如何来解决呢？
 @solution： 可不可以，给定一些语料库（越大越好），这些语料库中，抽出任意一个句子，来分析，发现，某个字符，要出现的原因是，跟他后面的字符出现是连再一起的，
 意思就是说他的出现，跟后面的字符出现存在某个规律，如果能找到一个方法，将他出不出现的概率给统计出来，以及他再整个语料库中出现的次数，从而随机生成出来的句子，就更符合
 随机规则。
 
 另外一个问题，某个字符，出现的概率，实际上再一个句子中出现的概率跟他再句子的位置由关系，如果一个字符出现再句首，那么这个句子由10，100，1000个字符，他跟句尾
 的出现几率联合在一起的概率就没有那么重要，同样的，跟倒数第二个字符也是一样，以此类推，我们只计算跟他紧接着的下一个的字符联合再一起出现的概率。
 
 要判断这个字符出不出现，只需要判断这个字符构成的词组出不出现以及他的概率。
 
 $$ language\_model(String) = Probability(String) \in (0, 1) $$
$$ Pro(w_1 w_2 w_3 w_4) = Pr(w_1 | w_2 w_3 w_ 4) * P(w2 | w_3 w_4) * Pr(w_3 | w_4) * Pr(w_4)$$
$$ Pro(w_1 w_2 w_3 w_4) \sim Pr(w_1 | w_2 ) * P(w2 | w_3 ) * Pr(w_3 | w_4) * Pr(w_4)$$

#语言模型 2_gram 模型
输入，是一个句子
输出，0，1直接的值，越近1，就句子越真实反之，就越不真实。


具体的做法：
第一步： 再语料库中分析全部字符，组成的一个单词出现的概率。
step1, 导入语料库
step2，将语料库进行清理，（主要去除掉invild字符,比如换行符，逗号，分号等）
step3，分词
step4, 计算每个分词出现的概率:每个单词，再总的单词中出现的概率大小
import pandas as pd
import re
import jieba
from collections import Counter

filename = '/Users/gaominquan/Downloads/sqlResult_1558435.csv'
content = pd.read_csv(filename,encoding='gb18030')
articles = content['content'].tolist

def token(string):
    return findall('\w+',string)

articles_clean = ''join(token(str(a)for a in articles))
    
def cut(string):
    return list(jieba.cut(string))

TOKEN= []
for i,line in enumerate(articles_clean):
    return TOKEN +=cut(line)

word_count = Counter(TOKEN)

def Prob_1(word):
    return word_count[word] / len(TOKEN)

第二步：
要求出一个单词跟另外一个单词的联合概率: 两个相邻单词合起来/再组合中的数量
TOKEN = [str(t) for t in TOKEN]

TOKEN_2_GRAM = ''.join(TOKEN[i:i+2] for i in range(len(TOKEN[:-2]))
word_count_2 = Counter(TOKEN_2_GRAM)

def prob_2(word1,word2):
    if word1 + word2 in word_count_2:
        return word_count_2[word1 + word2] / len(TOKEN_2_GRAM)
    else Prob_1(word1) + Prob_2(word2) / 2
    
第三步： 对于一句话的概率来说，可以先进行分词，然后就是相邻的两个单词的概率相乘起来。

def Prob_sentence(sentence):
    words = cut(sentence)
    
    for i, word in enumerate(words[:-1]):
        next_ = words[i+1]
        probability = prob_2(word,next_)
        sentence_prob *= probability
    return sentence_prob


综上。
要判断根据自动机产生的语句，他的出现这计划的概率，那么可以结合2_gram语言模型，进行判断：而不是根据语法进行判断的。
if sen in generate(gram=example_grammar,'sentence')
>>> '一个好看的小猫看着这个女人'

他的出现的概率就是，Prob_sentence(sen)

总结： 1. 自动机： 根据语法结构，终结符跟非终结符来循环递归生成一句话，多应用再机器语言的编译中。
2. 2_Gram 模型： 根据语料库的统计学中某个单词，某2个单词结合再一起的概率，统计出一句话是否符合概率的逻辑。越大表示话就是人话。
########################################################################################################################################
2. 基础理论部分
0. Can you come up out 3 sceneraies which use AI methods?
Ans: 机器翻译, 人形机器人, 智能客服系统，搜索推荐，自动驾驶，身份证识别等

1. How do we use Github; Why do we use Jupyter and Pycharm;
Ans: 1. github 用来存储代码，2. 对于开源的代码，可以进行下载学习甚至复用。 
使用Jupyter是用来演示，一些基础的数学公式方便展示。
使用pycharm是用做大型的项目开发用。

2. What's the Probability Model?
Ans:概率模型是在 统计学的基础上计算出某个事件的出现的可能性，用0到1的开区间来表示，越接近1的表示这件事情发生的概率越可能。

3. Can you came up with some sceneraies at which we could use Probability Model?
Ans: 撰写英语语言的自动提示，google搜索的自动补全，以及推荐搜索，智能客服等

4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?¶
Ans: 之所以运用概率模型，是因为，普通的自动机产生的语法不足以能表示人类的语言，产生的语句，需要可靠的语法树来维护，往往产生的语言，很不符合实际人类表述方式。
通过概率模型，可以比较轻松的将通过统计学的方法，将单词跟单词之间出现的概率计算出来，更符合逻辑表示。
而语法分析跟模式匹配，是随机的从规定好的语法中寻找单词进行匹配，往往有语法结构正确，但不符合人的逻辑表示。

5. What's the Language Model;
Ans:语言模型，就是通过统计学的方法将大量预料中，借助贝叶斯的公式，将一个单词出现的概率跟他临近的单词出现的概率想关联。提高了语言表述的准确性。

6. Can you came up with some sceneraies at which we could use Language Model?
Ans:google搜索中的自动补全，陪伴式机器人，智能客服

7. What's the 1-gram language model;
Ans:一个单词出现的概率跟他本身相关的语言模型，也就是单个字符或者单词出现的概率。

8. What's the disadvantages and advantages of 1-gram language model;
Ans: 1个单词粗线的概率跟他本身相关，不与是否有其他单词出现相关，优势在于很容易计算，用于统计词频。
劣势，应用比较窄，用做构成句子的话，可能完全不符合人类逻辑，

9. What't the 2-gram models;
Ans: 一个句子中，通过分词后，该单词出现的概率跟他临近一个单词出现的概率紧密相关。p（a|b)=p(ab)/p(b)
########################################################################################################################################
编程实践部分
1. 设计你自己的句子生成器
第一个语法：

human = """
human = 自己 寻找 活动
自己 = 我 | 俺 | 我们 
寻找 = 看看 | 找找 | 想找点
活动 = 乐子 | 玩的
"""


第二个语法：

host = """
host = 寒暄 报数 询问 业务相关 结尾 
报数 = 我是 数字 号 ,
数字 = 单个数字 | 数字 单个数字 
单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 
寒暄 = 称谓 打招呼 | 打招呼
称谓 = 人称 ,
人称 = 先生 | 女士 | 小朋友
打招呼 = 你好 | 您好 
询问 = 请问你要 | 您需要
业务相关 = 玩玩 具体业务
玩玩 = 耍一耍 | 玩一玩
具体业务 = 喝酒 | 打牌 | 打猎 | 赌博
结尾 = 吗？
"""


TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。
def create_grammar(grammar_str):
    grammar={}
    for line in grammar_str.split('\n'):
        if not line.strip(): continue
        stmt, exp = line.split('=')
        grammar[stmt.strip()] = [[e.strip()] for e in exp.split('|')]
    return grammar
    
def generate(gram, target):
    if target not in gram: return target
    expaned = [generate(gram, t) for t in random.choice(gram[target])]
    return ''.join(e for e in expaned if e!='null')    
 

TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:


def generate_n(n,gram,target):
    for i in n:
        generate(gram,target)
        

    
























      
 
 
 




































